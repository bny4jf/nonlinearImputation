\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{parskip}
\usepackage{setspace}
\usepackage{color}

\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\DeclareMathOperator*{\essup}{essup}

\title{Simple nonlinear imputation}
\author{}
\date{December 2020}

\begin{document}


\maketitle

\onehalfspacing


\section{Introduction}
In this paper we consider estimation in the linear index model where given a function $h$, the relationship between the LHS variable ($Y_i$) and RHS variables ($X_i \in \mathbb{R},\mathbf{Z_i} \in {1} \times \mathbb{R}^k$) is described by
\begin{align}
E[Y_i|X_i,\mathbf{Z_i}, M_i]&= h(\alpha X_i+ \mathbf{Z_i} \mathbf{\beta}),
\end{align}
and the variable $X_i$ has missing values whenever $M_i$, the missingness indicator is 1. We are exploring the properties of a simple imputation GMM estimator under the further assumption that the conditional distribution of $X_i$ given $\mathbf{Z}_i$ is independent of $M_i$:
\begin{align}
P[X_i <t|\mathbf{Z}_i=\mathbf{z}, M_i=m]= P[X_i <t|\mathbf{Z}_i=\mathbf{z}, M_i=m] \ \forall t \in \mathbb{R}.
\end{align}

\begin{example}[Probit model]
	The researcher has collected information for a binary outcome variable $Y$ and the $\mathbf{Z}$ for a large sample, but only has information about a control variable $X$ in a subsample.
	\begin{align*}
	Y_i&= \mathbf{1}[\alpha X_i + \mathbf{Z}_i\mathbf{\beta}>\epsilon_i]\\
	\epsilon_i&\sim N[0,1]
	\end{align*}
\end{example}

\begin{example}[Baseline censored linear model]
	We have that
	\begin{align*}
	Y_i^*&=\alpha X_i + \mathbf{Z}_i\mathbf{\beta}-\epsilon_i \\
	Y_i&= \mathbf{1}[Y_i^*>0]Y_i^* \\
	\epsilon_i&\sim N[0,1]
	\end{align*}
\end{example}
In both examples, we further assume that missingness is determined by the observable vector $\mathbf{Z}_i$. 

{\color{red} LITERATURE REVIEW, SELLING MISSING HERE}

\section{A simple imputation GMM estimator}

\subsection{Model assumptions and definition}
We collect our model assumptions for identification below. We denote the conditional pdf of $X_i$ given $\mathbf{Z_i}, M_i$ as $f_{x|z,m}$.
\begin{assumption}[Model] \label{ass_model}
We assume that
\begin{enumerate}
\item $E[Y_i|X_i=x,\mathbf{Z}_i=z, M_i=m]= h(\alpha x+z\beta)$, where $h$ is a known, strictly increasing and differentiable function,
\item $f_{x|z, m}(x,z,m)= f_{x|z}(x,z)$,
\item the support of the random variables $X_i, \mathbf{Z}_i$ does not lie in a  proper subspace of $\mathbb{R}^k$,
\item $P[M_i=1|X_i,\mathbf{Z}_i]<1$ $X_i,\mathbf{Z}_i-a.s.$
\end{enumerate}
\end{assumption}
The first two conditions in Assumption \ref{ass_model} are exclusion restrictions and restrict the missingness structure, but they are substantially weaker than the often used missing-at-random (MAR) assumption. Since we assume that $h$ is known, the first condition is also a functional form assumption in practice. The strict monotonicity of $h$ is a simple condition that together with the sufficient variation required ensures identification of the coefficient vector. Under Assumption \ref{ass_model}, the true $(\alpha, \mathbf{\beta})$ uniquely satisfies
\begin{align}
E[\tilde{g}(y,z,x,m; a,b)] = E\left[\begin{array}{c}
(1-M_i) X_i(Y_i- h(\alpha X_i+Z_i\beta))  \\
(1-M_i) Z_i(Y_i- h(\alpha X_i+Z_i\beta))
\end{array}\right]=0 \ a.s.,
\end{align}
where the expected value of $\tilde{g}$ is the population moment that is the basis of a consistent GMM estimator using the fully observed part of the sample. The first four arguments of $\tilde{g}$ are from the support of the corresponding random variables in our model. The fifth and sixth arguments are elements from  the (finite dimensional) parameter space of $\alpha, \beta$.

In addition to these identifying moments we also add imputation moments to our GMM estimator in order to increase efficiency. Define the vector-valued function $g$ as
\begin{align}
    g(y,z,x,m; a,b; E[y|z])=\left[\begin{array}{c}
        (1-m) x(y- h(a x+zb))  \\
        (1-m) z(y- h(a x+z b)) \\
          m z(y- E[y|z](z; a, b))
    \end{array}\right].
\end{align}
The function $g$ has the same same first arguments as $\tilde{g}$, while the last argument is a function that is supposed to estimate the conditional expectation of $Y_i$ given $\mathbf{Z}_i=z$. The function $E[y|z](.; \alpha, \beta): \mathbf{R}^k \rightarrow \mathbf{R}$ at the true values is defined by
\[E[y|z](z, \alpha, \beta)= E[Y_i| \mathbf{Z}_i=z, M_i=1]. 
\]
Using Assumption \ref{ass_model},
\begin{align}
    E[Y_i|\mathbf{Z_i}=z, M_i=1]=E[Y_i|\mathbf{Z_i}=z, M_i=0]= \int h(\alpha x + z \beta) f_{x|z}(x,z) dx,
\end{align}
so the definition of the infinite dimensional nuisance parameter becomes
\begin{align}
E[y|z](z; a, b) = \int h(a x + z b) f_{x|z}(x,z) dx.
\end{align}
Clearly, this function is identified, given the second exclusion restriction in Assumption \ref{ass_model}. Moreover, note that
\begin{align}
E[g(Y_i,\mathbf{Z}_i,X_i,M_i; a,b; E[y|z])]=0 \iff a=\alpha, b=\beta.
\end{align}

Define the sample analogues (for a sample of size $n$)
\begin{align}
    \hat{g}(a,b; \hat{E}[y|z])&=n^{-1}\sum_{i=1}^n g(y_i,z_i,x_i,m_i; a,b; \hat{E}[y|z])= \\
    &=n^{-1}\sum_{i=1}^n\left[\begin{array}{c}
        (1-m_i) x_i(y_i- h(ax_i+z_i b))  \\
        (1-m_i) z_i(y_i- h(ax_i+z_i b)) \\
          m_i z_i(y_i- \hat{E}[y|z](z_i; a,b))
    \end{array}\right], \nonumber %\\
%    \hat{E}[y_i|z_i]&= \int h(a x+b z_i) \hat{f}_{x|z}(x,z_i)dx,
\end{align}
where $\hat{E}_{y|z}$ is an estimator of the conditional expectation $E(y|z)$. At the end of this section we give the Nadaraya-Watson estimator as a specific example for a viable $\hat{E}(y|z)$. The imputation GMM estimator is minimizing
\begin{align}
    \hat{Q}_n(a,b)= \hat{g}(a,b;\hat{E}[y|z])'\hat{W}\hat{g}(a,b;\hat{E}[y|z])
\end{align}
with respect to $a,b$, where the $\hat{W}$ is a symmetric weighting matrix such that for some $W$ (positive definite)
\begin{align}
    \hat{W} \stackrel{p}{\rightarrow}W.
\end{align}

{\color{red} The fully observed GMM estimator that is based on the moment}
\begin{align}
    \hat{\tilde{g}}(a,b)&=n^{-1}\sum_{i=1}^n g(y_i,z_i,x_i, m_i; a,b)= \\
&=n^{-1}\sum_{i=1}^n\left[\begin{array}{c}
(1-m_i) x_i(y_i- h(ax_i+z_i b))  \\
(1-m_i) z_i(y_i- h(ax_i+z_i b))
\end{array}\right], \nonumber %\\
\end{align}
and defined as the M-estimator minimizing
\begin{align}
\hat{\tilde{Q}}_n(a,b)= \hat{\tilde{g}}(a,b)'\hat{\tilde{W}}\hat{\tilde{g}}(a,b)
\end{align}
with respect to $a,b$, where the $\hat{\tilde{W}}$ is a symmetric weighting matrix such that for some $\tilde{W}$ (positive definite)
\begin{align}
\hat{\tilde{W}} \stackrel{p}{\rightarrow}\tilde{W}.
\end{align}


\subsection{Asymptotic properties}
In the following arguments we closely follow Ichimura and Newey (2015) and Chernozhukov et al. (2018). We denote the estimator as a random variable by $\hat{\theta}_n$, while the true value $\theta= (\alpha,\beta)$. We also introduce the notation $\hat{G}(\theta;\hat{E}[y|z])$ for the derivative of $\hat{g}$ with respect to the parameter vector. Correspondingly, the derivative of $E[g]$ is denoted by $G$.

\begin{assumption}\label{ass_regulatory1}
	We place several smoothness assumptions on the structural functions.
	\begin{itemize}
		\item $f_{x|z}(x,z)$ is continuously differentiable and uniformly bounded,
		\item $h$ is continuously differentiable with bounded derivatives.
	\end{itemize}
	Further usual assumptions:
	\begin{enumerate}
		\item random sampling,
		\item $\hat{E}[y|z](a,b)\stackrel{p}{\rightarrow} E[y|z](a,b)$ uniformly over $z$ and $(a,b) \in A \times B$,
		\item $\alpha,\beta \in A \times B$, a compact set,
		\item $G'WG$ is a.s. an invertible matrix.
	\end{enumerate}
\end{assumption}

Writing up the first order Taylor expansion of $\hat{g}$ around $\theta$ gives
\begin{align}
    0&=\hat{G}'(\hat{\theta}_n;\hat{E}[y|z])\hat{W}\hat{g}(\hat{\theta}_n;\hat{E}[y|z])= \\ &=\hat{G}'(\hat{\theta}_n;\hat{E}[y|z])\hat{W}\hat{g}(\theta;\hat{E}[y|z])+ \hat{G}'(\hat{\theta}_n;\hat{E}[y|z])\hat{W}\hat{G}(\bar{\theta}_n;\hat{E}[y|z])(\hat{\theta}_n-\theta)= \nonumber \\
    &= \hat{G}'\hat{W}\hat{g}_0+ \hat{G}'\hat{W}\bar{G}(\hat{\theta}_n-\theta). \nonumber
    \end{align}
For legibility, we abbreviated the notation for the various matrices from the second row. Here $\bar{\theta}$ is a vector of convex combinations of $\theta$ and $\hat{\theta}_n$, but this value can (and generally should) be different for different rows of $\hat{G}$. In this sense we abuse the notation for $\hat{G}$ somewhat when we say it is evaluated at $\bar{\theta}$. We also introduce
\begin{align}
\hat{g}_0=\hat{g}(\theta;\hat{E}[y|z]), \\
g_0 = g(Y_i, X_i, \mathbf{Z}_i, M_i; \theta; E[y|z]).
\end{align}

Given Assumption \ref{ass_regulatory1}, we can prove that $\hat{\theta}_n$ consistently estimates $\theta$, as $||\hat{G}'\hat{W}\bar{G}||$ is going to be bounded and $\hat{g}_0 \rightarrow E[g_0]=0$ with probability approaching 1. This in turn yields
\begin{align}
    \hat{\theta}_n -\theta&=-(\hat{G}'\hat{W}\bar{G})^{-1} \hat{G}'\hat{W} \hat{g}_0 \\
    &= -(G'WG)^{-1}G'W \hat{g}_0 + o_p(\hat{g}_0).\nonumber
\end{align}

The following is a consequence of Theorem 7 of Ichimura and Newey (2015), and our calculations in the previous subsection.
\begin{proposition}\label{prop_asympTheory}
	Under Assumption \ref{ass_model}-\ref{ass_regulatory1}, if $||E[y|z](z_i, a,b)-\hat{E}[y|z](z_i, a,b)||=o_p(n^{-1/4}) \ Z_i-a.s.$ and for every $a,b$, then
	\[\sqrt{n}(\theta_n-\theta) \stackrel{d}{\rightarrow} N[0, (G'WG)^{-1}G'W \Omega W G (G'WG)^{-1}],
	\]
	with $\Omega= E[g_0'g_0]$.
\end{proposition}
We also note that the fully observed GMM estimator has the same properties under our assumptions.

\emph{Convergence rate of $\hat{E}[y|z]$ for the Nadaraya-Watson estimator}
We are going to estimate the conditional expectation by a Nadaraya-Watson type estimator.
\begin{align}
    \hat{E}[y|z](y_i, z_i; a,b)= \frac{\sum_j K[H^{-1}(z_i-z_j)] h(a x_j + b z_i)}{\sum_j K[H^{-1}(z_i-z_j)]}.
\end{align}
For the sake of simplicity, we will assume that $H$ is a diagonal matrix with positive diagonal entries. Let us have the entry that decreases to zero at the slowest rate denoted by $h_{max}$, moreover let us write $\prod h_k= h$.

\begin{assumption}\label{ass_estimation}
    Our estimation assumptions:
    \begin{enumerate}
        \item $h_{max}\rightarrow 0$
        \item $nh_{max}^{k_z}\rightarrow \infty$
        \item $K$ is a Parzen-Rosenblatt kernel (second order)
        \item $Supp(Z_i)$ is compact, with the strong pdf assumption
        \item the pdf for $Z_i$ is twice differentiable
        \item the conditional distribution function $f_{x|z}(x,z)$ is twice differentiable (with bounded Hessian)
    \end{enumerate}
\end{assumption}
Some of these assumptions are stronger than necessary (notably, conditions number 2 and number 4). We conclude that as long as the bandwidth $h_{max}$ is $o(n^{-1/4})$, we only have to worry about the contribution of the variances, under the restriction that the $\hat{E}[y|z]$ converges uniformly to the conditional expectation as a function, which gives the restriction that $nh \rightarrow \infty$. For these conditions we need that the $k<4$ that is involved in the calculations of the conditional expectations. In addition, we note that discrete $z_i$-s are allowed with simple modifications of the estimator and theory.
\begin{corollary}\label{cor_NWTheory}
	If $\hat{E}[y|z]$ is the Nadaraya-Watson estimator with $k<4$, under Assumption \ref{ass_model}-\ref{ass_estimation} the imputation GMM estimator $\hat{\theta}_n$ is such that
		\[\sqrt{n}(\theta_n-\theta) \stackrel{d}{\rightarrow} N[0, (G'WG)^{-1}G'W \Omega W G (G'WG)^{-1}],
	\]
	with $\Omega= E[g_0'g_0]$. 
\end{corollary}
The corollary suggests that the dimensionality of $\mathbf{Z}_i$ included in the is crucial for imputation to work. There are two reasons to include an (always observed) RHS variable into the imputation moments:
\begin{itemize}
	\item Weakening of the missing-at-random assumption: we think the variable is related to missingness,
	\item Predictive power for $X_i$: observing the variable gives information about the missing RHS variable
\end{itemize}
Even if the second point would not warrant an inclusion of a particular element of $\mathbf{Z}_i$ into the group of conditioning variables in $E[y|z]$, if we think that it may be related to missingness, it needs to be included in the estimator. 

\subsection{The role of the weighting matrix and efficiency}
Our goal is to minimize the Mean Squared-Error (MSE). It can be calculated as the expected value of the diagonal of the matrix
\begin{align}
(\hat{\theta}_n &-\theta)(\hat{\theta}_n -\theta)' \\
&=((G'WG)^{-1}G'W \hat{g}_0 + o_p(\hat{g}_0))((G'WG)^{-1}G'W \hat{g}_0 + o_p(\hat{g}_0))'= \nonumber \\
&= (G'WG)^{-1} G'W \hat{\Omega}_0 W G (G'WG)^{-1}  + o_p(\hat{g}_0\hat{g}_0'). \nonumber
\end{align}

Now let us set
\[W^{-1}=\hat{\Omega}_0=n^{-1}\sum_i g(y_i,z_i,x_i,m_i; \alpha,\beta; \hat{E}[y|z]) g(y_i,z_i,x_i,m_i; \alpha,\beta; \hat{E}[y|z])',
\]
then we get that
\begin{align}
diag&\left((\hat{\theta}_n -\theta)(\hat{\theta}_n -\theta)'\right)= diag\left((G'\hat{\Omega} G)^{-1} + o_p(\hat{\Omega}^{-1})\right).
\end{align}
We note that
\begin{align*}
diag\left((G'WG)^{-1} G'W \hat{\Omega}W G (G'WG)^{-1}-  (G'\hat{\Omega} G)^{-1}\right) + o_p(\hat{g}_0\hat{g}_0')> 0 \ ev.,
\end{align*}
which means this is the infeasible optimal weighting for large samples. This optimal weighting matrix can be estimated by the inverse of $(g\hat{g}')$, which is a block-diagonal matrix.
\begin{align}
\hat{\Omega} = \left[\begin{array}{cc}
\hat{\tilde{\Omega}} & 0 \\ 0 & \hat{B}
\end{array}\right].
\end{align}
The block matrix corresponding to the imputation moments ($\hat{B}$) is positive definite if the additional moments do not have a zero optimal weight as $n$ tends to infinity. In this case $diag(G\hat{W}G)^{-1}$ is smaller or equal than the diagonal of the optimal covariance matrix of the estimator that does not contain the added moments (which is $(\tilde{G}'^{-1}\hat{\tilde{\Omega}} \tilde{G})$).
\begin{assumption}\label{ass_optimalWeighting}
	$\hat{W} = \hat{\Omega}^{-1}.$
\end{assumption}
\begin{proposition}
	Under Assumption \ref{ass_model}-\ref{ass_regulatory1} and \ref{ass_optimalWeighting}, $MSE(\hat{\theta}_n)\leq MSE(\tilde{\theta}_n) \ ev.$ for any admissible weighting of the $\tilde{\theta}_n$ estimator. The inequality is strict for the $\beta$-coefficients corresponding to the fully observed variables ($z_i$).
\end{proposition}

\begin{remark}
	Under our assumptions, the relative optimal weight for the $q$th and $r$th element of $\hat{g}$ (denoted by superscripts) has the same order as
	\[\frac{L_2(\hat{g}_0^q-g_0^q)}{L_2(\hat{g}_0^r-g_0^r)}.
	\]
	So if an element of $\hat{g}_0$ does not converge with a $\sqrt{n}$ rate to zero due to the estimates nuisance parameter, its relative weight is set to be arbitrarily close to zero, eventually. The optimal weighting matrix selects the moments automatically so that the estimator is always root-n consistent with an asymptotic variance-covariance matrix that is the same as for the fully observed GMM estimator.\\
	However, this inference introduces additional noise and loss of degrees of freedom in finite samples, so if the applied researcher implements the imputation method for $k\geq 4$, imputation will \textit{increase} standard errors. 
\end{remark}

\begin{remark}
	Our estimation method and theory can be easily extended to the case when there are more than one RHS variables are missing. However, we do not pursue the full imputation estimator where the researcher uses two variables with missing values to predict each other (`Swiss cheese case').
\end{remark}

\section{Monte Carlo simulation}


\section{Application}


\section{Conclusion}

\section{References}

Abrevaya and Donald (2017)
Ichimura and Newey (2015)
Pakes and Pollard (1989)
Chernozhukov et al. (2018)

\section{Appendix}

\subsection{Proposition 1}
Theorem 7 checking the assumptions?

\subsection{Corollary 1}
\begin{align}
\hat{E}(zy|z=z_i)= (nh)^{-1}\frac{\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)}{(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)]},
\end{align}
where the denominator clearly converges in probability to $f(z_i)$, uniformly, so we are going to ignore it, and focus on the expected value of
\begin{align}
(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)-E[z g(\alpha x + \beta z)|z=z_i].
\end{align}
First, let us calculate
\begin{align}
&E[(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)|\mathbf{z}]= \\
&=(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i E[g(\alpha x_j + \beta z_i)|\mathbf{z}]= \nonumber \\
&= (nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i \int g(\alpha x + \beta z_i) f_{x|z}(x,z_j) dx,\nonumber
\end{align}
which gives
\begin{align}
&E[(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)-E[z_i g(\alpha x + \beta z_i)|z_i]|\mathbf{z}]= \\
&=(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i \int g(\alpha x + \beta z_i) (f_{x|z}(x,z_j)-f_{x|z}(x,z_i)) dx. \nonumber
\end{align}
It is interesting that it is only the conditional distribution that has the discrepancy. Taking now expectation w.r.t. $z_j$ as well,
\begin{align}
&E[(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)-E[z_i g(\alpha x + \beta z_i)|z_i]|z_i]= \\
&=h^{-1}\int K[H^{-1}(z_i-z)] z_i \int g(\alpha x + \beta z_i) (f_{x|z}(x,z)-f_{x|z}(x,z_i)) f(z) dxdz= \nonumber \\
&= \int K[\Delta z] (z_i \int g(\alpha x+\beta z_i)Df_{x|z}(x,z_i) dx \Delta z (f(z_i) + Df(\bar{z})\cdot \Delta z \cdot H ) d \Delta z + \nonumber\\
&+ \int \Delta z' H  D^2f_{x|z}(x,\bar{\bar{z}})H \Delta z dx \nonumber
\end{align}
after taking a second-order Taylor expansion in $f(z)$ and estimating $f_{x|z}(x,z)-f_{x|z}(x,z_i)$ similarly, finally, substituting $\Delta z= H^{-1}(z_i-z)$ for integration. By our boundedness assumptions, this is going to be bounded uniformly over $z_i$.

Given that we have  second order kernel, we collect the terms and take integrals (everything has the same rate uniformly over $z_i$)
\begin{align}
&E[(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)-E[z_i g(\alpha x + \beta z_i)|z_i]]= \\
&=O(h_{max}^2) \nonumber
\end{align}

{\color{red} (rewrite this), but checked}

\subsection{Proposition 2}
Before we would start, we prove that the infeasible optimal weighting matrix is indeed optimal.


Now we prove Proposition 2.

\end{document}
