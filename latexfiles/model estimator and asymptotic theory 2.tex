\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{parskip}
\usepackage{setspace}
\usepackage{color}

\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\DeclareMathOperator*{\essup}{essup}

\title{Simple nonlinear imputation}
\author{}
\date{December 2020}

\begin{document}


\maketitle

\onehalfspacing


\section{Introduction}
In this paper we consider GMM estimation in models where given a known function $g_0$, the LHS variable ($Y_i$) and RHS variables ($X_i \in \mathbb{R},Z_i \in {1} \times \mathbb{R}^k$) there is a set of {\color{red}differentiable} population moment functions $g_0$ that are zero in expectation if and only if they are evaluated at the true parameter values ($\beta$),
\begin{align}
E[g_0(Y_i, X_i, Z_i,\beta)|X_i,Z_i] = 0,
\end{align}
where the vector $\mathbf{\beta}\in \mathbb{R}^{p}$ is what the researcher aims to estimate.

The key problem in our setting is that the scalar $X_i$ variable is missing whenever $M_i$, the missingness indicator is 1. We are exploring the properties of a simple imputation GMM estimator under the further assumption that the conditional distribution of $X_i$ and $Y_i$ given $Z_i$ is independent of $M_i$. 

\begin{example}[Probit Non-linear Least Squares]
	The researcher collected data of a binary outcome variable $Y$ and the vector of independent variables $Z$ for a large sample, but only has information about a control variable $X$ in a subsample. The following model connects the observables:
	\begin{align}
	Y_i&= \mathbf{1}[\alpha X_i + Z_i\gamma>\epsilon_i] \label{eq_probitEx1}\\
	\epsilon_i&\sim N[0,1] \label{eq_probitEx2}
	\end{align}
	In the following we denote the cumulative distribution function of the standard normal distribution by $\Phi$ and the probability density function by $\phi$. One way of estimating the model is non-linear least squares, when the set of population moments is
	\[E\left[\begin{array}{c}
	X_i(Y_i-\Phi(aX_i+Z_ic))\\
	Z_i(Y_i-\Phi(aX_i+Z_ic))
	\end{array}
	\right].
	\]
\end{example}

\begin{example}[Probit Maximum Likelihood]
	Assume that the data generating process is the same as in the previous example (equations \eqref{eq_probitEx1}-\eqref{eq_probitEx2}). Another way to estimate the coefficients is based on the likelihood principle that corresponds to the population moment
	\begin{align*}
	E\left[\begin{array}{c}
	X_i\left(Y_i \frac{\phi(aX_i+Z_ic)}{\Phi(aX_i+Z_ic)} - (1-Y_i)\frac{\phi(aX_i+Z_ic)}{1-\Phi(aX_i+Z_ic)}\right) \\
	Z_i\left(Y_i \frac{\phi(aX_i+Z_ic)}{\Phi(aX_i+Z_ic)} - (1-Y_i)\frac{\phi(aX_i+Z_ic)}{1-\Phi(aX_i+Z_ic)}\right)
	\end{array}
	\right].
	\end{align*}
\end{example}
In both examples, our key exclusion restrictions are satisfied when the missingness is caused by the always observed variables $Z_i$, but conditional on these, $Y_i$ and $X_i$ are not related to it.

Given our setup, it is possible to estimate the parameter of interest using the completely observed records in the data based on the population moment $g_0$ (the complete case GMM estimator). We show when it is advantageous to augment the original moments using simple imputation to increase the efficiency of the estimates. 

{\color{red} LITERATURE REVIEW, SELLING MISSING HERE}

\section{A simple imputation GMM estimator}

\subsection{Model assumptions and estimators}
We denote the conditional pdf of $X_i$ given $Z_i, M_i$ as $f_{x|z,m}$. Let us denote the set of random vectors with the same support as $Supp(X_i,Z_i)$ as $\mathcal{W}$.
\begin{assumption}[Model] \label{ass_model}
There exists a known differentiable $g_0$ vector-valued function such that
\begin{enumerate}
\item $E[g_0(Y_i,X_i, Z_i, b)] = 0 \iff b=\beta$,
\item $E[g_0(Y_i, X_i, Z_i, b)|X_i, Z_i, M_i] = 0 \Leftarrow b=\beta,$
\item $P[M_i=1|X_i,Z_i]<1$ $X_i,Z_i-a.s.$
\item $Z_i$ can be partitioned into $Z_i= [Z^0_i, Z^1_i]$, such that conditional on $Z_i^1$, the distribution of $X_i, Z^0_i$ is the same whether $M_i=0$ or $M_i=1$.
\end{enumerate}
\end{assumption}
The first condition in Assumption \ref{ass_model} ensures identification in the case without missingness. The second and third conditions are the usual assumptions for the validity of the \textit{complete case GMM} estimator\footnote{See the definition below.} under missing-at-random (MAR), when the researcher simply omits the observations with missing values. The fourth condition is the weakened version of the missing-at-completely-random assumption often assumed by researchers, but it is a stronger assumption than MAR. These conditions are satisfied if the $(Y,X_i,Z^0_i)$ vector is independent of $M_i$, conditional on $Z_i^1$. Further we denote the dimension of the support of $Z^1_i$ by $k_1$.

Next we define three GMM estimators: 
\begin{enumerate}
	\item The infeasible \emph{full-data GMM estimator} with the moment
	\[E[g_0(Y_i, Z_i, X_i; \beta)]=0,
	\]
	\item The \emph{complete case GMM estimator} that is based on the population moment
	\begin{align}
	E[\tilde{g}(Y_i, Z_i, X_i, M_i; \beta)] = E\left[(1-M_i) g_0(Y_i, X_i,Z_i; \beta )\right]=0 \ a.s.
	\end{align}
	for $\tilde{g}: Supp(Y_i, Z_i, X_i, M_i) \times B$ for $B \subset \mathbb{R}^p,$
	\item The \emph{imputation GMM estimator} with population moment
	\begin{align}
	E[g(Y_i,Z_i,X_i,M_i; b; e)]=\left[\begin{array}{c}
	(1-M_i) g_0(Y_i,X_i,Z_i,b)]\\
	M_i e(Y_i, Z_i^1; b)
	\end{array}\right],
	\end{align}
	with $e: \mathbb{R}^{k_1}\times B \rightarrow \mathbb{R}$
	\begin{align}
	e(y, z^1,b)= E[g_0(Y_i, X_i,Z_i, b)|Y_i=y, Z_i=z].
	\end{align}
\end{enumerate}

The infeasible \emph{full-data GMM estimator} has the sample moment
\begin{align}
\hat{g}_0(y_i,z_i,x_i;b) = n^{-1}\sum_{i=1}^n g_0(y_i,x_i,z_i,b)
\end{align}
and given a weighting matrix $\hat{W}_0\stackrel{p}{\rightarrow} W_0$ (positive definite) minimizes
\begin{align}
\hat{Q}^0_n(b)= \hat{g}_0(b)'\hat{W}_0\hat{g}_0(b)
\end{align}
with respect to $b$.

The \textit{complete case estimator} is the result of a usual strategy of omitting the observations with missing values. This estimator is based on the moment
\begin{align}
\hat{\tilde{g}}(b)&=n^{-1}\sum_{i=1}^n \tilde{g}(y_i,z_i,x_i, m_i; b)= \\
&=n^{-1}\sum_{i=1}^n
(1-m_i) g_0(y_i,z_i,x_i; b), \nonumber %\\
\end{align}
and defined as the M-estimator minimizing
\begin{align}
\hat{\tilde{Q}}_n(b)= \hat{\tilde{g}}(b)'\hat{\tilde{W}}\hat{\tilde{g}}(b)
\end{align}
with respect to $b$, where the $\hat{\tilde{W}}$ is a symmetric weighting matrix such that for some $\tilde{W}$ (positive definite)
\begin{align}
\hat{\tilde{W}} \stackrel{p}{\rightarrow}\tilde{W}.
\end{align}
Under Assumption \ref{ass_model} this estimator is consistent. 

In addition to the feasible identifying moments $\tilde{g}$ we also add imputation moments to our \emph{imputation GMM estimator} in order to increase efficiency. The function $g$ has the same first arguments as $\tilde{g}$, while the last argument is a function that represents the conditional expectation of $g_0(Y_i, X_i, Z_i, b)$ given $Z_i=z$ and $Y_i=y$. Clearly, this function is identified from the complete subsample. Define the sample analogues (for a sample of size $n$)
\begin{align}
    \hat{g}(b, \hat{e})&=n^{-1}\sum_{i=1}^n g(y_i,z_i,x_i,m_i; b; \hat{e})= \\
    &=n^{-1}\sum_{i=1}^n\left[\begin{array}{c}
        (1-m_i) g_0(y_i,z_i,x_i; b)  \\
         m_i \hat{e}(y_i, z_i; b))
    \end{array}\right], \nonumber
\end{align}
where $\hat{e}$ is an estimator of the conditional expectation $e$. At the end of this section we give the Nadaraya-Watson estimator as a specific example for a viable $\hat{e}$. The \textit{imputation GMM estimator} is minimizing
\begin{align}
    \hat{Q}_n(b)= \hat{g}(b;\hat{e})'\hat{W}\hat{g}(b;\hat{e})
\end{align}
with respect to $b$, where the $\hat{W}$ is a symmetric weighting matrix such that for some $W$ (positive definite)
\begin{align}
    \hat{W} \stackrel{p}{\rightarrow}W.
\end{align}

\subsection{Asymptotic properties}
In the following arguments we closely follow Ichimura and Newey (2015) and Chernozhukov et al. (2018). We denote the imputation estimator as a random variable by $\hat{\beta}_n$, while the true value $\beta$. We also introduce the notation $\hat{G}(\beta;\hat{e})$ for the derivative of $\hat{g}$ with respect to the parameter vector. Correspondingly, the derivative of $E[g]$ is denoted by $G$.

\begin{assumption}\label{ass_regulatory1}
	We make several regulatory assumptions.
	\begin{itemize}
%		\item $f_{x|z}(x,z)$ is continuously differentiable and uniformly bounded,
		\item $g_0$ is continuously differentiable with bounded derivatives,
		\item random sampling,
		\item $\beta \in B$, a compact set,
		\item $G'WG$ is a.s. an invertible matrix.
		\item $\hat{e}(y,z, b)\stackrel{p}{\rightarrow} e(y,z, b)$ uniformly,
		\item $\sup_{y,z}\hat{e}(y,z, b)<\infty$ almost surely for all $b \in B$,
	\end{itemize}
\end{assumption}

Writing up the first order Taylor expansion of $\hat{g}$ around $\beta$ gives
\begin{align}
    0&=\hat{G}'(\hat{\beta}_n;\hat{e})\hat{W}\hat{g}(\hat{\beta}_n;\hat{e})= \\ &=\hat{G}'(\hat{\beta}_n;\hat{e})\hat{W}\hat{g}(\beta;\hat{e})+ \hat{G}'(\hat{\beta}_n;\hat{e})\hat{W}\hat{G}(\bar{\beta}_n;\hat{e})(\hat{\beta}_n-\beta)= \nonumber \\
    &= \hat{G}'\hat{W}\hat{g}_1+ \hat{G}'\hat{W}\bar{G}(\hat{\beta}_n-\beta). \nonumber
    \end{align}
For legibility, we abbreviated the notation for the various matrices from the second row. Here $\bar{\beta}_n$ is a vector of convex combinations of $\beta$ and $\hat{\beta}_n$, but this value can (and generally should) be different for different rows of $\hat{G}$. In this sense we abuse the notation for $\hat{G}$ somewhat when we say it is evaluated at $\bar{\beta}$. We also introduce
\begin{align}
\hat{g}_1&=\hat{g}(\beta;\hat{e}), \\
g_1 &= g(Y_i, X_i, Z_i, M_i; \beta; e).
\end{align}

Given Assumption \ref{ass_regulatory1}, we can prove that $\hat{\beta}_n$ consistently estimates $\beta$, as $||\hat{G}'\hat{W}\bar{G}||$ is going to be bounded and $\hat{g}_1 \rightarrow E[g_1]=0$ with probability approaching 1. This in turn yields
\begin{align}
    \hat{\beta}_n -\beta&=-(\hat{G}'\hat{W}\bar{G})^{-1} \hat{G}'\hat{W} \hat{g}_1 \\
    &= -(G'WG)^{-1}G'W \hat{g}_1 + o_p(\hat{g}_1).\nonumber
\end{align}

The following proposition .
\begin{proposition}\label{prop_asympTheory}
	Under Assumption \ref{ass_model}-\ref{ass_regulatory1}, if $\sup_{y,z}E|e(y, z,b)-\hat{e}(y,z,b)||=o_p(n^{-1/2})$, then
	\[\sqrt{n}(\theta_n-\theta) \stackrel{d}{\rightarrow} N[0, (G'WG)^{-1}G'W \Omega W G (G'WG)^{-1}],
	\]
	with $\Omega= \lim E[\hat{g}_1'\hat{g}_1]$.
\end{proposition}
We also note that the complete case GMM estimator has the analogous properties under our assumptions.

\emph{Convergence rate of $\hat{e}(y,z,b)$ for the Nadaraya-Watson estimator}
We are going to estimate the conditional expectation by a Nadaraya-Watson type estimator. For the sake of simplicity, we assume that all observables are continuously distributed. 
\begin{align}
    \hat{e}(y_i, z_i; b)= \frac{\sum_j K[H^{-1}(\tilde{z}_i-\tilde{z}_j)] g_0(y_i, x_j z_i)}{\sum_j K[H^{-1}(\tilde{z}_i-\tilde{z}_j)]}.
\end{align}
For the sake of simplicity, we will assume that $H$ is a diagonal matrix with positive diagonal entries. Let us have the entry that decreases to zero at the slowest rate denoted by $h_{max}$, moreover let us write $\prod h_k= h$.

\begin{assumption}\label{ass_estimation}
    Our estimation assumptions:
    \begin{enumerate}
        \item $h_{max}\rightarrow 0$
        \item $nh_{max}^{k_z}\rightarrow \infty$
        \item $K$ is a Parzen-Rosenblatt kernel (second order)
        \item $Supp(Z_i)$ is compact, with the strong pdf assumption
        \item the pdf for $Z_i$ is twice differentiable
        \item the conditional distribution function $f_{x|z}(x,z)$ is twice differentiable (with bounded Hessian)
    \end{enumerate}
\end{assumption}
Some of these assumptions are stronger than necessary (notably, conditions number 2 and number 4). We conclude that as long as the bandwidth $h_{max}$ is $o(n^{-1/4})$, we only have to worry about the contribution of the variances, under the restriction that the $\hat{e}$ converges uniformly to the conditional expectation as a function, which gives the restriction that $nh \rightarrow \infty$. For these conditions we need that the $\tilde{k}<4$ that is involved in the calculations of the conditional expectations. In addition, we note that discrete $z_i$-s are allowed with simple modifications of the estimator and theory.
\begin{corollary}\label{cor_NWTheory}
	If $\hat{e}$ is the Nadaraya-Watson estimator with $\tilde{k}<4$, under Assumption \ref{ass_model}-\ref{ass_estimation} the imputation GMM estimator $\hat{\beta}_n$ is such that
		\[\sqrt{n}(\beta_n-\beta) \stackrel{d}{\rightarrow} N[0, (G'WG)^{-1}G'W \Omega W G (G'WG)^{-1}],
	\]
	with $\Omega= E[\hat{g}_1'\hat{g}_1]$. 
\end{corollary}
The corollary suggests that the dimensionality of $Z_i$ included in the conditioning is crucial for imputation to work. There are two reasons to include an (always observed) RHS variable into the imputation moments:
\begin{itemize}
	\item Weakening of the missing-at-random assumption: we think the variable is related to missingness,
	\item Predictive power for $X_i$: observing the variable gives information about the missing RHS variable
\end{itemize}
Even if the second point would not warrant an inclusion of a particular element of $Z_i$ into the group of conditioning variables in $\hat{e}$, if we think that it may be related to missingness, it needs to be included in the estimator.

Next we are going to analyze our two examples and also show how with certain type of added imputation moment we can decrease the effective number of dimensions we need to condition on. This also connects this paper with the original strategy of Abrevaya and Donald (2017).

\emph{Example 1: Probit with NLS}

The imputation moment we add is equivalent to
\begin{align}
E\left[\begin{array}{c}
M_i E[X_i|Z^1_i, Y_i](Y_i-E[\Phi(aX_i+Z_ic)|Z_i^1,Y_i])\\
M_i E[Z_i^0|Z^1_i, Y_i](Y_i-E[\Phi(aX_i+Z_ic)|Z_i^1,Y_i]) \\
M_i Z^1_i(Y_i-E[\Phi(aX_i+Z_ic)|Z_i^1,Y_i])
\end{array}
\right].
\end{align}
However, in order to preserve the simplicity of the approach, we only choose to focus on the moments that are additive in $X_i$ and $Y_i$. If a particular row of $g_0$ is such for some known $h_1,h_2$, then
\begin{align}
E&[M_i E[g_0(Y_i,X_i,Z_i,b)|Y_i,Z_i^1]]= \\
&= E[M_iE[h_1(Y_i,Z_i)+h_2(X_i,Z_i)|Y_i,Z_i^1]]= \nonumber \\
&= E[M_iE[E[h_1(Y_i,Z_i)+h_2(X_i,Z_i)|Y_i,Z_i]|Y_i,Z_i^1]]= \nonumber \\
&= E[M_i h_1(Y_i,Z_i)] +E[M_i E[h_2(X_i,Z_i)|Y_i, Z_i^1]]= \nonumber \\
&=E[M_i h_1(Y_i,Z_i)] +E[E[M_i E[h_2(X_i,Z_i)|Y_i, Z_i^1]|Z_i^1]]= \nonumber \\
&=E[M_i h_1(Y_i,Z_i)] +E[E[M_i|Z_i^1] E[h_2(X_i,Z_i)|Z_i^1]]= \nonumber \\
&=E[M_i h_1(Y_i,Z_i)] +E[P[M_i=1|Z_i]E[h_2(X_i,Z_i)|Z_i^1]]= \nonumber \\
&= E[M_i h_1(Y_i,Z_i)] +E[M_i E[h_2(X_i,Z_i)|Z^1_i]]= 
\end{align}
due to the Law of Iterated Expectations and because
\begin{align}
E[E[ h_2(X_i,Z_i)|Y_i, Z_i^1]|M_i=1] = E[P[M_i=1]E[h_2(X_i,Z_i)|Y_i, Z_i^1, M_i=1] + \\
+P[M_i=0]E[h_2(X_i,Z_i)|Y_i, Z_i^1, M_i=0]|M_i=1] 
\end{align}

 conditional on $Z_i^1$, we have that missingness is independent of $X_i,Z_i^0$. The only unobserved variable we need to take the expectation over is $X_i$, since $Z_i$ and $Y_i$ is observed even if $M_i=1$.




\emph{Example 2: Probit with Maximum Likelihood}


\subsection{The role of the weighting matrix and efficiency}
Our goal is to minimize the Mean Squared-Error (MSE). It can be calculated as the expected value of the diagonal of the matrix
\begin{align}
(\hat{\theta}_n &-\theta)(\hat{\theta}_n -\theta)' \\
&=((G'WG)^{-1}G'W \hat{g}_0 + o_p(\hat{g}_0))((G'WG)^{-1}G'W \hat{g}_0 + o_p(\hat{g}_0))'= \nonumber \\
&= (G'WG)^{-1} G'W \hat{\Omega}_0 W G (G'WG)^{-1}  + o_p(\hat{g}_0\hat{g}_0'). \nonumber
\end{align}

Now let us set
\[W^{-1}=\hat{\Omega}_0=n^{-1}\sum_i g(y_i,z_i,x_i,m_i; \alpha,\beta; \hat{E}[y|z]) g(y_i,z_i,x_i,m_i; \alpha,\beta; \hat{E}[y|z])',
\]
then we get that
\begin{align}
diag&\left((\hat{\theta}_n -\theta)(\hat{\theta}_n -\theta)'\right)= diag\left((G'\hat{\Omega} G)^{-1} + o_p(\hat{\Omega}^{-1})\right).
\end{align}
We note that
\begin{align*}
diag\left((G'WG)^{-1} G'W \hat{\Omega}W G (G'WG)^{-1}-  (G'\hat{\Omega} G)^{-1}\right) + o_p(\hat{g}_0\hat{g}_0')> 0 \ ev.,
\end{align*}
which means this is the infeasible optimal weighting for large samples. This optimal weighting matrix can be estimated by the inverse of $(g\hat{g}')$, which is a block-diagonal matrix.
\begin{align}
\hat{\Omega} = \left[\begin{array}{cc}
\hat{\tilde{\Omega}} & 0 \\ 0 & \hat{B}
\end{array}\right].
\end{align}
The block matrix corresponding to the imputation moments ($\hat{B}$) is positive definite if the additional moments do not have a zero optimal weight as $n$ tends to infinity. In this case $diag(G\hat{W}G)^{-1}$ is smaller or equal than the diagonal of the optimal covariance matrix of the estimator that does not contain the added moments (which is $(\tilde{G}'^{-1}\hat{\tilde{\Omega}} \tilde{G})$).
\begin{assumption}\label{ass_optimalWeighting}
	$\hat{W} = \hat{\Omega}^{-1}.$
\end{assumption}
\begin{proposition}
	Under Assumption \ref{ass_model}-\ref{ass_regulatory1} and \ref{ass_optimalWeighting}, $MSE(\hat{\theta}_n)\leq MSE(\tilde{\theta}_n) \ ev.$ for any admissible weighting of the $\tilde{\theta}_n$ estimator. The inequality is strict for the $\beta$-coefficients corresponding to the fully observed variables ($z_i$).
\end{proposition}

\begin{remark}
	Under our assumptions, the relative optimal weight for the $q$th and $r$th element of $\hat{g}$ (denoted by superscripts) has the same order as
	\[\frac{L_2(\hat{g}_0^q-g_0^q)}{L_2(\hat{g}_0^r-g_0^r)}.
	\]
	So if an element of $\hat{g}_0$ does not converge with a $\sqrt{n}$ rate to zero due to the estimates nuisance parameter, its relative weight is set to be arbitrarily close to zero, eventually. The optimal weighting matrix selects the moments automatically so that the estimator is always root-n consistent with an asymptotic variance-covariance matrix that is the same as for the fully observed GMM estimator.\\
	However, this inference introduces additional noise and loss of degrees of freedom in finite samples, so if the applied researcher implements the imputation method for $k\geq 4$, imputation will \textit{increase} standard errors. 
\end{remark}

\begin{remark}
	Our estimation method and theory can be easily extended to the case when there are more than one RHS variables are missing. However, we do not pursue the full imputation estimator where the researcher uses two variables with missing values to predict each other (`Swiss cheese case').
\end{remark}

\section{Monte Carlo simulation}


\section{Application}


\section{Conclusion}

\section{References}

Abrevaya and Donald (2017)
Ichimura and Newey (2015)
Pakes and Pollard (1989)
Chernozhukov et al. (2018)

\section{Appendix}

\subsection{Proposition 1}
Theorem 7 checking the assumptions?

\subsection{Corollary 1}
\begin{align}
\hat{E}(zy|z=z_i)= (nh)^{-1}\frac{\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)}{(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)]},
\end{align}
where the denominator clearly converges in probability to $f(z_i)$, uniformly, so we are going to ignore it, and focus on the expected value of
\begin{align}
(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)-E[z g(\alpha x + \beta z)|z=z_i].
\end{align}
First, let us calculate
\begin{align}
&E[(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)|\mathbf{z}]= \\
&=(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i E[g(\alpha x_j + \beta z_i)|\mathbf{z}]= \nonumber \\
&= (nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i \int g(\alpha x + \beta z_i) f_{x|z}(x,z_j) dx,\nonumber
\end{align}
which gives
\begin{align}
&E[(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)-E[z_i g(\alpha x + \beta z_i)|z_i]|\mathbf{z}]= \\
&=(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i \int g(\alpha x + \beta z_i) (f_{x|z}(x,z_j)-f_{x|z}(x,z_i)) dx. \nonumber
\end{align}
It is interesting that it is only the conditional distribution that has the discrepancy. Taking now expectation w.r.t. $z_j$ as well,
\begin{align}
&E[(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)-E[z_i g(\alpha x + \beta z_i)|z_i]|z_i]= \\
&=h^{-1}\int K[H^{-1}(z_i-z)] z_i \int g(\alpha x + \beta z_i) (f_{x|z}(x,z)-f_{x|z}(x,z_i)) f(z) dxdz= \nonumber \\
&= \int K[\Delta z] (z_i \int g(\alpha x+\beta z_i)Df_{x|z}(x,z_i) dx \Delta z (f(z_i) + Df(\bar{z})\cdot \Delta z \cdot H ) d \Delta z + \nonumber\\
&+ \int \Delta z' H  D^2f_{x|z}(x,\bar{\bar{z}})H \Delta z dx \nonumber
\end{align}
after taking a second-order Taylor expansion in $f(z)$ and estimating $f_{x|z}(x,z)-f_{x|z}(x,z_i)$ similarly, finally, substituting $\Delta z= H^{-1}(z_i-z)$ for integration. By our boundedness assumptions, this is going to be bounded uniformly over $z_i$.

Given that we have  second order kernel, we collect the terms and take integrals (everything has the same rate uniformly over $z_i$)
\begin{align}
&E[(nh)^{-1}\sum_j K[H^{-1}(z_i-z_j)] z_i g(\alpha x_j + \beta z_i)-E[z_i g(\alpha x + \beta z_i)|z_i]]= \\
&=O(h_{max}^2) \nonumber
\end{align}

{\color{red} (rewrite this), but checked}

\subsection{Proposition 2}
Before we would start, we prove that the infeasible optimal weighting matrix is indeed optimal.


Now we prove Proposition 2.

\end{document}
